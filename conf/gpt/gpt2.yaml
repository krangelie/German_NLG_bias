

name: "gpt2"

path: "gpt2-large" # EN
# "benjamin/gerpt2-large" # GER


do_sample: True
top_p: 0.92 # set of words whose cumulative prob. > p. The prob. mass
  # is then redistributed among this set of words.
top_k: 0
temperature: 0.7 # [x/low_temp for x in logits] - creates sharper distribution
max_length: 40
num_return_sequences: 1


