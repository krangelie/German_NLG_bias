name: 'transformer'

# for tuning
hyperparameters:
  batch_size: 
    name: "batch_size"
    choices: [8, 16, 32, 64]

  learning_rate: 
    name: "lr"
    lower: 0.00001
    upper: 0.01

  n_hidden_lin:
    name: "n_hidden_lin"
    choices: [64, 128, 256, 512]

  n_hidden_lin_2:
    name: "n_hidden_lin_2"
    choices: [0, 64, 128, 256, 512]

  dropout:
    name: "dropout"
    lower: 0.0
    upper: 0.9
    step: 0.1

unanimous:
  batch_size: 64
  dropout: 0.30000000000000004
  lr: 0.0004952563793931958
  n_hidden_lin: 128
  n_hidden_lin_2: 0
  n_output: 3
  n_epochs: 100
  patience: 20
  model_path: ""

majority:
  batch_size: 32
  dropout: 0.4
  lr: 0.00006
  n_hidden_lin: 256
  n_hidden_lin_2: 64
  n_output: 3
  n_epochs: 100
  patience: 10
  model_path: models/EN/checkpoint-300
  #models/EN/checkpoint-300
  #models/GER/sbert_regard_classifier_model.pth
  #"/home/angelie/Documents/PhD/FAccT2022/NLG_bias/mlruns/0/caddfdf95fdf46dc942a0de306f3aad5
  #/artifacts/model/data/model.pth"
  #models/GER/sbert_regard_classifier_model.pth
  #models/EN/checkpoint-300 #models/GER/sbert_regard_classifier_model.pth
  #models/${language}
